{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Using cached fasttext-0.9.2.tar.gz (68 kB)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/anaconda3/lib/python3.7/site-packages (from fasttext) (2.6.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/anaconda3/lib/python3.7/site-packages (from fasttext) (46.0.0.post20200309)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from fasttext) (1.18.1)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-macosx_10_15_x86_64.whl size=339740 sha256=ed6e4fe9ca86bc45b9ddc8612dd950fb80819a305a74183d270dc87555ede53b\n",
      "  Stored in directory: /Users/luda-gordeeva/Library/Caches/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import expit\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "from catboost.utils import eval_metric\n",
    "from scipy.special import expit \n",
    "from scipy.spatial import distance\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "article_words = {\"the\", \"a\", \"an\"}\n",
    "negation_words = {\"no\", \"not\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGUAGE = \"ru\"\n",
    "LANGUAGE = \"en\"\n",
    "\n",
    "VECTORS = {} # cache\n",
    "CLF = LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\", random_state=0, max_iter=10000, tol=1e-5)  # solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "#fasttext.util.download_model(LANGUAGE)\n",
    "ft = fasttext.load_model(f'cc.{LANGUAGE}.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    def __init__(self, expression, lang, order):\n",
    "        self.expression = expression\n",
    "        self.lang = lang\n",
    "        self.order = order\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.expression(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = {\n",
    "    \"Bias\": Feature(\n",
    "        expression=lambda word1, word2, analyzer: 1,\n",
    "        lang={\"ru\", \"en\"},\n",
    "        order=0\n",
    "    ),\n",
    "    \"Words equality\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "            int(word1 == word2),\n",
    "\n",
    "        lang={\"ru\", \"en\"},\n",
    "        order=1\n",
    "    ),\n",
    "    \"Words inequality\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "            int(word1 != word2),\n",
    "\n",
    "        lang={\"ru\", \"en\"},\n",
    "        order=2\n",
    "    ),\n",
    "    \"Lemmas equality\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "            int(analyzer.parse(word1)[0].normal_form == analyzer.parse(word2)[0].normal_form),\n",
    "\n",
    "        lang={\"ru\"},\n",
    "        order=3\n",
    "    ),\n",
    "    \"Lemmas inequality\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "            int(analyzer.parse(word1)[0].normal_form != analyzer.parse(word2)[0].normal_form),\n",
    "\n",
    "        lang={\"ru\"},\n",
    "        order=4\n",
    "    ),\n",
    "    \"Words stem equality\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "            int(analyzer.stem(word1) == analyzer.stem(word2)),\n",
    "\n",
    "        lang={\"en\"},\n",
    "        order=5\n",
    "    ),\n",
    "    \"Words stem inequality\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "            int(analyzer.stem(word1) == analyzer.stem(word2)),\n",
    "\n",
    "        lang={\"en\"},\n",
    "        order=6\n",
    "    ),    \n",
    "    \"Word length difference\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        abs(len(word1) - len(word2)),\n",
    "\n",
    "        lang={\"ru\", \"en\"},\n",
    "        order=7\n",
    "    ),\n",
    "    \"Word length difference norm by max\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        abs(len(word1) - len(word2)) / max(len(word1), len(word2)),\n",
    "\n",
    "        lang={\"ru\", \"en\"},\n",
    "        order=8\n",
    "    ),\n",
    "\n",
    "    \"Levenshtein distance between words\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        Levenshtein.distance(word1, word2),\n",
    "\n",
    "        lang={\"ru\", \"en\"},\n",
    "        order=9\n",
    "    ),\n",
    "    \"Levenshtein distance between lemmas\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        Levenshtein.distance(analyzer.parse(word1)[0].normal_form, analyzer.parse(word2)[0].normal_form),\n",
    "\n",
    "        lang={\"ru\"},\n",
    "        order=10\n",
    "    ),\n",
    "    \"Levenshtein distance between lemmas norm by the sum of word lengths\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        Levenshtein.distance(analyzer.parse(word1)[0].normal_form, analyzer.parse(word2)[0].normal_form) /\n",
    "        (len(analyzer.parse(word1)[0].normal_form) + len(analyzer.parse(word2)[0].normal_form)),\n",
    "\n",
    "        lang={\"ru\"},\n",
    "        order=11\n",
    "    ),\n",
    "    \"Levenshtein distance word stems\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        Levenshtein.distance(analyzer.stem(word1), analyzer.stem(word2)),\n",
    "\n",
    "        lang={\"en\"},\n",
    "        order=12\n",
    "    ),\n",
    "    \"Word similarity\": Feature(\n",
    "        expression=lambda word1, word2, analyzer: 1 - distance.cosine(ft.get_word_vector(word1), ft.get_word_vector(word2)) if word1 and word2 else 1,\n",
    "\n",
    "        lang={\"ru\", \"en\"},\n",
    "        order=13\n",
    "    ),\n",
    "    \"Reference in dictionary\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        analyzer.parse(word1)[0].is_known,\n",
    "\n",
    "        lang={\"ru\"},\n",
    "        order=14\n",
    "    ),\n",
    "    \"Hypothesis in dictionary\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        analyzer.parse(word2)[0].is_known,\n",
    "\n",
    "        lang={\"ru\"},\n",
    "        order=15\n",
    "    ),\n",
    "    \"E with dots\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        word1.replace('ё', 'е') == word2.replace('ё', 'е'),\n",
    "\n",
    "        lang={\"ru\"},\n",
    "        order=16\n",
    "    ),\n",
    "    \"Error in no word\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        word1 != word2 and \"не\" in {word1, word2},\n",
    "\n",
    "        lang={\"ru\"},\n",
    "        order=17\n",
    "    ),\n",
    "    \"Insertion\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        word1 == \"\",\n",
    "\n",
    "        lang={\"ru\", \"en\"},\n",
    "        order=18\n",
    "    ),\n",
    "    \"Deletion\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        word2 == \"\",\n",
    "\n",
    "        lang={\"ru\", \"en\"},\n",
    "        order=19\n",
    "    ),\n",
    "    \"Stop-word\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        int(word1 in stop_words),\n",
    "\n",
    "        lang={\"en\"},\n",
    "        order=20\n",
    "    ),\n",
    "    \"IsArticle\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        int(word1 != word2 and word1 in article_words and word2 in article_words),\n",
    "\n",
    "        lang={\"en\"},\n",
    "        order=21\n",
    "    ),\n",
    "    \"IsNegation\": Feature(\n",
    "        expression=lambda word1, word2, analyzer:\n",
    "        int(word1 != word2 and (word1 in negation_words or word2 in negation_words)),\n",
    "\n",
    "        lang={\"en\"},\n",
    "        order=22\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MERa Model**\n",
    "\n",
    "    lang: language (ru or en)\n",
    "    mode: feature selection mode\n",
    "        default: all features for the selected language\n",
    "        black_list: features will be rejected\n",
    "        white_list: features will be included\n",
    "    features: list of features for the black_list or white_list mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERa:\n",
    "    def select_features(self, features):\n",
    "        selected = set()\n",
    "\n",
    "        for feature in FEATURES:\n",
    "            if self.mode == \"default\" \\\n",
    "                    or self.mode == \"white_list\" and feature in features \\\n",
    "                    or self.mode == \"black_list\" and feature not in features:\n",
    "                selected.add(feature)\n",
    "        return selected\n",
    "\n",
    "    def __init__(self, lang, mode=\"default\", features=set()):\n",
    "        if lang not in {\"ru\", \"en\"}:\n",
    "            raise Exception(f\"Invalid language {lang}\")\n",
    "        self.lang = lang\n",
    "\n",
    "        self.analyzer = MorphAnalyzer() if lang == \"ru\" else PorterStemmer()\n",
    "\n",
    "        if mode not in [\"default\", \"black_list\", \"white_list\"]:\n",
    "            raise Exception(\"Incorrect mode\")\n",
    "        self.mode = mode\n",
    "\n",
    "        if features | FEATURES.keys() != FEATURES.keys():\n",
    "            raise Exception(f\"Following features are not defined: {(features | FEATURES.keys()) - FEATURES.keys()}\")\n",
    "\n",
    "        self.features = sorted(list(self.select_features(features)), key=lambda x: FEATURES[x].order)\n",
    "        self.size = len(self.features)\n",
    "\n",
    "        self.weights = [0, 0, 1] + [0] * (self.size - 2)  # WER by default\n",
    "\n",
    "    def __call__(self, ref, hyp, show=False, *args, **kwargs):\n",
    "        ref_words, hyp_words = tokenizer_split(ref, hyp)\n",
    "        pairs, costs = optimize(ref_words, hyp_words, self.cost)\n",
    "        diff_ref, diff_hyp, diff_cost = alignment(pairs, costs)\n",
    "\n",
    "        value = self.weights[0] + sum(costs) / max(len(ref_words), len(hyp_words))\n",
    "        value = expit(value)\n",
    "\n",
    "        if show:\n",
    "            print(diff_ref)\n",
    "            print(diff_hyp)\n",
    "            print(diff_cost)\n",
    "\n",
    "        return value, {\n",
    "            \"diff_ref\": diff_ref,\n",
    "            \"diff_hyp\": diff_hyp,\n",
    "            \"diff_cost\": diff_cost\n",
    "        }\n",
    "\n",
    "    def vector(self, word1, word2):\n",
    "        \"\"\"\n",
    "        Feature map (only if there is no cache)\n",
    "        \"\"\"\n",
    "        if (word1, word2) in VECTORS:\n",
    "            return VECTORS[(word1, word2)]\n",
    "        x = [0] * self.size\n",
    "        index = 0\n",
    "        for feature in self.features:\n",
    "            x[index] = FEATURES[feature](word1, word2, self.analyzer)\n",
    "            index += 1\n",
    "        VECTORS[(word1, word2)] = x\n",
    "        return x\n",
    "\n",
    "    def cost(self, word1, word2):\n",
    "        return np.array(self.vector(word1, word2)).T.dot(np.array(self.weights[1:]))\n",
    "\n",
    "    def fit(self, X_texts, y, probs, index_train, index_test, mode=\"meaning loss\", save_path=None):\n",
    "        global VECTORS, CLF \n",
    "        VECTORS = {}\n",
    "        CLF = LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\", random_state=0, max_iter=10000, tol=1e-5)\n",
    "    \n",
    "        assert set(index_train) | set(index_test) == set(range(len(y)))\n",
    "        old_mean, old_std = np.zeros((self.size + 1,)), np.ones((self.size + 1,))\n",
    "        old_weights = self.weights\n",
    "        last_norm = 0\n",
    "        for _ in range(10):\n",
    "            X, old_mean, old_std = self.e_step(X_texts, old_mean, old_std) if mode == \"meaning loss\" else self.e_step_sbs(X_texts, old_mean, old_std)\n",
    "            old_weights = self.weights\n",
    "            \n",
    "            self.m_step(X[index_train], np.array(y)[index_train], np.array(probs)[index_train])\n",
    "            show_scores(index_train, index_test, X, probs, self.weights, self.features)\n",
    "\n",
    "            print(norm(self.weights - old_weights))   \n",
    "            if abs(norm(self.weights - old_weights) - last_norm) < 0.01:\n",
    "                break\n",
    "                \n",
    "            last_norm = norm(self.weights - old_weights)\n",
    "\n",
    "        self.weights = unstandard_weights(X, self.weights, old_mean, old_std)\n",
    "        if save_path:\n",
    "            save_coeff([\"Sentence bias\"] + self.features, self.weights, save_path)\n",
    "\n",
    "    def e_step(self, X_texts, old_mean, old_std):\n",
    "        X = np.ones((len(X_texts), self.size))\n",
    "        i = 0\n",
    "        for ref, hyp in X_texts:\n",
    "            pairs, costs = optimize(*tokenizer_split(ref, hyp), self.cost)\n",
    "#             # uncomment for logging\n",
    "#             alignment(pairs, costs)\n",
    "            X[i] = (np.sum([self.vector(word1, word2) for word1, word2 in pairs], 0) / max(len(ref), len(hyp)))\n",
    "            i += 1\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        X, new_mean, new_std, self.weights = standard(X, self.weights, old_mean, old_std)\n",
    "        return X, new_mean, new_std\n",
    "    \n",
    "    def e_step_sbs(self, X_texts, old_mean, old_std):\n",
    "        X = np.ones((len(X_texts), self.size))\n",
    "        i = 0\n",
    "        for ref, hyp_left, hyp_right in X_texts:\n",
    "            pairs_left, costs_left = optimize(*tokenizer_split(ref, hyp_left), self.cost)\n",
    "            pairs_right, costs_right = optimize(*tokenizer_split(ref, hyp_right), self.cost)\n",
    "#             # uncomment for logging\n",
    "#             alignment(pairs, costs)\n",
    "            X[i] = -(np.sum([self.vector(word1, word2) for word1, word2 in pairs_right], 0) / max(len(ref), len(hyp_right))) + \\\n",
    "            (np.sum([self.vector(word1, word2) for word1, word2 in pairs_left], 0) / max(len(ref), len(hyp_left)))\n",
    "            i += 1\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        X, new_mean, new_std, self.weights = standard(X, self.weights, old_mean, old_std)\n",
    "        return X, new_mean, new_std\n",
    "    \n",
    "\n",
    "    def m_step(self, X, y, probs):\n",
    "        CLF.fit(X, y, sample_weight=[probs[i][y[i]] for i in range(len(y))])\n",
    "        self.weights = CLF.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_coeff(first_column, second_column, path):\n",
    "        with open(path, \"w\") as f:\n",
    "            first_column_size = max([len(row) for row in first_column])\n",
    "            for i in range(len(first_column)):\n",
    "                f.write(f'{first_column[i]}{\" \" * (first_column_size - len(first_column[i]))}: {round(second_column[i], 4)}\\n')\n",
    "\n",
    "def tokenizer_split(ref, hyp):\n",
    "    return ref.split(), hyp.split()\n",
    "\n",
    "\n",
    "def optimize(words_ref, words_hyp, cost):\n",
    "    # --> define dynamic table size\n",
    "    n = len(words_ref)\n",
    "    m = len(words_hyp)\n",
    "\n",
    "    # --> create table\n",
    "    table = np.full((n + 1, m + 1), np.inf)\n",
    "    table[0, 0] = 0\n",
    "\n",
    "    # --> fill table\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            # --> deletion\n",
    "            table[i + 1, j] = min(table[i + 1][j], table[i][j] + cost(words_ref[i], \"\"))\n",
    "\n",
    "            # --> insertion\n",
    "            table[i, j + 1] = min(table[i][j + 1], table[i][j] + cost(\"\", words_hyp[j]))\n",
    "\n",
    "            # --> substitution\n",
    "            table[i + 1, j + 1] = min(table[i + 1][j + 1],\n",
    "                                      table[i][j] + cost(words_ref[i], words_hyp[j]))\n",
    "\n",
    "    # --> fill table edge\n",
    "    for j in range(m):\n",
    "        # --> only insertion possible\n",
    "        table[n, j + 1] = min(table[n][j + 1], table[n][j] + cost(\"\", words_hyp[j]))\n",
    "\n",
    "    for i in range(n):\n",
    "        # --> deletion\n",
    "        table[i + 1, m] = min(table[i + 1][m], table[i][m] + cost(words_ref[i], \"\"))\n",
    "\n",
    "    ### RECONSTRUCTION ###\n",
    "    eps = 1e-10\n",
    "    pairs = []\n",
    "    costs = []\n",
    "    i, j = n, m\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        new_i, new_j = i, j\n",
    "\n",
    "        if i > 0:\n",
    "            # --> deletion\n",
    "            if abs(table[i - 1, j] + cost(words_ref[i - 1], \"\") - table[i, j]) < eps:\n",
    "                pair = (words_ref[i - 1], \"\")\n",
    "                new_i, new_j = i - 1, j\n",
    "\n",
    "        if i > 0 and j > 0:\n",
    "            # --> substitution\n",
    "            if abs(table[i - 1, j - 1] + cost(words_ref[i - 1], words_hyp[j - 1]) - table[i, j]) < eps:\n",
    "                pair = (words_ref[i - 1], words_hyp[j - 1])\n",
    "                new_i, new_j = i - 1, j - 1\n",
    "\n",
    "        if j > 0:\n",
    "            # --> insertion\n",
    "            if abs(table[i, j - 1] + cost(\"\", words_hyp[j - 1]) - table[i, j]) < eps:\n",
    "                pair = (\"\", words_hyp[j - 1])\n",
    "                new_i, new_j = i, j - 1\n",
    "\n",
    "        pairs.append(pair)\n",
    "        costs.append(cost(pair[0], pair[1]))\n",
    "        i, j = new_i, new_j\n",
    "\n",
    "    # --> reverse pairs\n",
    "    pairs.reverse()\n",
    "    costs.reverse()\n",
    "\n",
    "    return pairs, costs\n",
    "\n",
    "\n",
    "def alignment(pairs, costs):\n",
    "\n",
    "    diff_ref = []\n",
    "    diff_hyp = []\n",
    "    diff_cost = []\n",
    "\n",
    "    for i in range(len(costs)):\n",
    "        ref, hyp = pairs[i]\n",
    "        cost = str(round(costs[i], 3))\n",
    "\n",
    "        length = max(len(ref), len(hyp), len(cost))\n",
    "        diff_ref.append(ref + \"*\" * (length - len(ref)))\n",
    "        diff_hyp.append(hyp + \"*\" * (length - len(hyp)))\n",
    "        diff_cost.append(cost + \" \" * (length - len(cost)))\n",
    "\n",
    "    return ' '.join(diff_ref), ' '.join(diff_hyp), ' '.join(diff_cost)\n",
    "\n",
    "\n",
    "def train_test_split(size, p=0.8):\n",
    "    indexes = list(range(size))\n",
    "    random.shuffle(indexes)\n",
    "    k = round(size * p)\n",
    "    return indexes[:k], indexes[k:]\n",
    "\n",
    "\n",
    "def show_scores(index_train, index_test, X, probs, weights, features):\n",
    "    auc_test = bootstrap(1000, 0.75,\n",
    "                         np.array(probs)[:, 1][index_test],\n",
    "                         expit(X[index_test].dot(np.array(weights))),\n",
    "                         lambda a1, a2: eval_metric(a1, a2, 'AUC'))  # на классе 1\n",
    "    auc_train = bootstrap(1000, 0.75,\n",
    "                          np.array(probs)[:, 1][index_train],\n",
    "                          expit(X[index_train].dot(np.array(weights))),\n",
    "                          lambda a1, a2: eval_metric(a1, a2, 'AUC'))\n",
    "    print(\"Standard weights\\n\")\n",
    "    print(\"sent const:\", weights[0])\n",
    "    print(feature_weights_table(features, weights[1:]))\n",
    "    print(\"AUC test\", auc_test)\n",
    "    print(\"AUC train\", auc_train)\n",
    "\n",
    "\n",
    "def bootstrap(k, p, seq1, seq2, func):\n",
    "    \"\"\"\n",
    "\n",
    "    :param k: samples count\n",
    "    :param p: samples part\n",
    "    :param seq1, seq2: two list for comparing\n",
    "    :param func: score function\n",
    "    :return: interval\n",
    "    \"\"\"\n",
    "\n",
    "    values = []\n",
    "    n = round(len(seq1) * p)\n",
    "    for i in range(k):\n",
    "        indexes = resample(list(range(len(seq1))), n_samples=n)\n",
    "        values.append(func(seq1[indexes], seq2[indexes]))\n",
    "    values.sort()\n",
    "    tails = int(0.025 * len(values))\n",
    "    return values[tails], values[-tails]\n",
    "\n",
    "\n",
    "def feature_weights_table(first_column, second_column):\n",
    "    first_column_size = max([len(row) for row in first_column])\n",
    "    for i in range(len(first_column)):\n",
    "        print(f'{first_column[i]}{\" \" * (first_column_size - len(first_column[i]))}: {round(second_column[i], 4)}')\n",
    "\n",
    "\n",
    "def standard(x, weights, old_mean, old_std):\n",
    "    new_mean = np.mean(x, axis=0)\n",
    "    new_std = np.std(x, axis=0)\n",
    "\n",
    "    x_std = x - new_mean\n",
    "    x_std /= new_std\n",
    "\n",
    "    # --> consts\n",
    "    for i in range(1, x.shape[1]):\n",
    "        if not new_std[i] or not old_std[i] or (set(x[:, i]) | {0, 1}) == {0, 1}:\n",
    "            x_std[:, i] = x[:, i]\n",
    "        else:\n",
    "            weights[0] -= weights[i] * (old_mean[i] / old_std[i] - new_mean[i] / old_std[i])\n",
    "            weights[i] *= new_std[i] / old_std[i]\n",
    "\n",
    "    x_std[:, 0] = np.ones((x.shape[0],))\n",
    "    return x_std, new_mean, new_std, weights\n",
    "\n",
    "\n",
    "def unstandard_weights(x, weights, old_mean, old_std):\n",
    "    for i in range(1, weights.shape[0]):\n",
    "        if old_std[i] and (set(x[:, i]) | {0, 1} != {0, 1}):\n",
    "            weights[0] -= weights[i] * old_mean[i] / old_std[i]\n",
    "            weights[i] /= old_std[i]\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meaning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_ml(path):\n",
    "    desc_to_votes = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    for markup in data:\n",
    "        desc = (\n",
    "            markup[\"reference\"],\n",
    "            markup[\"hypothesis\"],\n",
    "            markup[\"recognition_id\"],\n",
    "            markup[\"dataset\"],\n",
    "            markup[\"record\"],\n",
    "            markup[\"model\"],\n",
    "            str(markup[\"noise\"][\"level_idx\"]) if markup[\"noise\"] is not None else None,\n",
    "        )\n",
    "\n",
    "        if desc not in desc_to_votes:\n",
    "            desc_to_votes[desc] = (0, 0)\n",
    "\n",
    "        votes_total, votes_ok = desc_to_votes[desc]\n",
    "\n",
    "        votes_total += 1\n",
    "        if markup[\"ok\"]:\n",
    "            votes_ok += 1\n",
    "\n",
    "        desc_to_votes[desc] = (votes_total, votes_ok)\n",
    "    return [{\n",
    "        \"reference\": desc[0],\n",
    "        \"hypothesis\": desc[1],\n",
    "        \"votes_total\": desc_to_votes[desc][0],\n",
    "        \"votes_ok\": desc_to_votes[desc][1],\n",
    "    } for desc in desc_to_votes]\n",
    "\n",
    "def prepare_data_ml():\n",
    "    data = aggregate_ml(\"votes_check_raw.json\")\n",
    "    \n",
    "    X = []\n",
    "    for item in data:\n",
    "        ref = item[\"reference\"]\n",
    "        ref = \"\" if type(ref) == float else ref\n",
    "\n",
    "        hyp = item[\"hypothesis\"]\n",
    "        hyp = \"\" if type(hyp) == float else hyp\n",
    "        X.append((ref, hyp))\n",
    "    y = [1 if item[\"votes_ok\"]/item[\"votes_total\"] < 0.5 else -1 for item in data]  # не правильно 1, правильно -1\n",
    "    probs = []\n",
    "    alpha = 0.5\n",
    "    for item in data:\n",
    "        p = (item[\"votes_ok\"] + alpha) / (item[\"votes_total\"] + 2 * alpha)\n",
    "        probs.append([p, 1 - p])\n",
    "\n",
    "    return X, y, probs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, probs = prepare_data_ml()\n",
    "# size = 100\n",
    "# X, y, probs = X[:size], y[:size], probs[:size]\n",
    "index_train, index_test = train_test_split(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 11284\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset size: {len(X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 8.138628659892788e-14\n",
      "Bias            : -0.0381\n",
      "Words inequality: 1.0345\n",
      "None\n",
      "AUC test ([0.7556062707952622], [0.7743076475686829])\n",
      "AUC train ([0.7626682309091144], [0.7722889663293212])\n",
      "0.997968528966293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 4.7684078907650473e-14\n",
      "Bias            : -0.0388\n",
      "Words inequality: 1.0347\n",
      "None\n",
      "AUC test ([0.7547862013997131], [0.7744926472382014])\n",
      "AUC train ([0.7624432444824282], [0.7718993657427033])\n",
      "0.0006537806875331318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 4.7684078907650473e-14\n",
      "Bias            : -0.0388\n",
      "Words inequality: 1.0347\n",
      "None\n",
      "AUC test ([0.7548134672577067], [0.773990988871138])\n",
      "AUC train ([0.7622870805050236], [0.7724811316754475])\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"WER\")\n",
    "m = MERa(\"en\", \"white_list\", features={\n",
    "    \"Bias\",\n",
    "    \"Words inequality\"\n",
    "})\n",
    "m.fit(X, y, probs, index_train, index_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MERa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MERa(\"en\", \"white_list\", features={\n",
    "    \"Bias\",\n",
    "    \"Words inequality\",\n",
    "    \"Insertion\",\n",
    "    \"Deletion\",\n",
    "    \"Words stem inequality\",\n",
    "    \"Word length difference\",\n",
    "    \"Levenshtein distance between words\",\n",
    "    \"Levenshtein distance word stems\",\n",
    "    \"Stop-word\",\n",
    "    \"Word similarity\",\n",
    "    \"IsArticle\",\n",
    "    \"IsNegation\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 1.861863204880396e-06\n",
      "Bias                              : 1.3753\n",
      "Words inequality                  : -0.4953\n",
      "Words stem inequality             : -0.0503\n",
      "Word length difference            : -0.5814\n",
      "Levenshtein distance between words: 0.7834\n",
      "Levenshtein distance word stems   : -0.0468\n",
      "Word similarity                   : -1.9375\n",
      "Insertion                         : 0.209\n",
      "Deletion                          : 0.2929\n",
      "Stop-word                         : 0.1565\n",
      "IsArticle                         : -0.0333\n",
      "IsNegation                        : 0.0061\n",
      "None\n",
      "AUC test ([0.7661609239120406], [0.7841083354814438])\n",
      "AUC train ([0.7758048864466123], [0.7843872654620101])\n",
      "2.65372385325419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 8.872123286149565e-09\n",
      "Bias                              : 0.6943\n",
      "Words inequality                  : -0.2366\n",
      "Words stem inequality             : -0.0948\n",
      "Word length difference            : 0.8912\n",
      "Levenshtein distance between words: 0.6784\n",
      "Levenshtein distance word stems   : -0.606\n",
      "Word similarity                   : -0.7444\n",
      "Insertion                         : -0.2195\n",
      "Deletion                          : 0.0486\n",
      "Stop-word                         : 0.0019\n",
      "IsArticle                         : 0.0\n",
      "IsNegation                        : -0.0948\n",
      "None\n",
      "AUC test ([0.7187749970791677], [0.7423149097390087])\n",
      "AUC train ([0.7257886816511769], [0.7374470363694346])\n",
      "4.947400790817684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 7.451427491796217e-09\n",
      "Bias                              : 1.3727\n",
      "Words inequality                  : -0.6137\n",
      "Words stem inequality             : -0.0186\n",
      "Word length difference            : -0.3079\n",
      "Levenshtein distance between words: 0.7124\n",
      "Levenshtein distance word stems   : 0.0157\n",
      "Word similarity                   : -1.8973\n",
      "Insertion                         : 0.1877\n",
      "Deletion                          : 0.2179\n",
      "Stop-word                         : 0.1562\n",
      "IsArticle                         : -0.0386\n",
      "IsNegation                        : 0.0065\n",
      "None\n",
      "AUC test ([0.7666788685808763], [0.7848226421785778])\n",
      "AUC train ([0.7759153956350809], [0.7848039647238276])\n",
      "2.2532596657158335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: -2.3715157369985803e-13\n",
      "Bias                              : 1.4901\n",
      "Words inequality                  : -0.6131\n",
      "Words stem inequality             : 0.1026\n",
      "Word length difference            : -0.2227\n",
      "Levenshtein distance between words: 0.8881\n",
      "Levenshtein distance word stems   : 0.57\n",
      "Word similarity                   : -1.859\n",
      "Insertion                         : 0.444\n",
      "Deletion                          : 0.0981\n",
      "Stop-word                         : 0.1411\n",
      "IsArticle                         : 0.0\n",
      "IsNegation                        : -0.0017\n",
      "None\n",
      "AUC test ([0.7648579264889958], [0.783330459409169])\n",
      "AUC train ([0.7743992583635688], [0.7833470974588266])\n",
      "1.2937840354410324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 1.1924969659436082e-07\n",
      "Bias                              : 1.3625\n",
      "Words inequality                  : -0.5772\n",
      "Words stem inequality             : 0.0776\n",
      "Word length difference            : -0.4363\n",
      "Levenshtein distance between words: 0.6799\n",
      "Levenshtein distance word stems   : 0.1495\n",
      "Word similarity                   : -1.8743\n",
      "Insertion                         : 0.2722\n",
      "Deletion                          : 0.288\n",
      "Stop-word                         : 0.156\n",
      "IsArticle                         : -0.0055\n",
      "IsNegation                        : 0.0084\n",
      "None\n",
      "AUC test ([0.766905866214356], [0.7852651564202962])\n",
      "AUC train ([0.776215931121469], [0.7850764278650106])\n",
      "0.782434684254137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 3.3894507410770625e-14\n",
      "Bias                              : 1.4798\n",
      "Words inequality                  : -0.5891\n",
      "Words stem inequality             : 0.1019\n",
      "Word length difference            : -0.1436\n",
      "Levenshtein distance between words: 0.8086\n",
      "Levenshtein distance word stems   : 0.601\n",
      "Word similarity                   : -1.8404\n",
      "Insertion                         : 0.4489\n",
      "Deletion                          : 0.1035\n",
      "Stop-word                         : 0.1414\n",
      "IsArticle                         : 0.0\n",
      "IsNegation                        : -0.0013\n",
      "None\n",
      "AUC test ([0.7660216478787678], [0.7835025982962716])\n",
      "AUC train ([0.7746717529803577], [0.7833913574125548])\n",
      "1.4518106877136978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 9.312648562183595e-11\n",
      "Bias                              : 1.3369\n",
      "Words inequality                  : -0.5504\n",
      "Words stem inequality             : 0.0927\n",
      "Word length difference            : -0.4287\n",
      "Levenshtein distance between words: 0.6346\n",
      "Levenshtein distance word stems   : 0.1906\n",
      "Word similarity                   : -1.8675\n",
      "Insertion                         : 0.2595\n",
      "Deletion                          : 0.2777\n",
      "Stop-word                         : 0.156\n",
      "IsArticle                         : -0.0054\n",
      "IsNegation                        : 0.0085\n",
      "None\n",
      "AUC test ([0.7668027304685249], [0.785019112997917])\n",
      "AUC train ([0.7762468443232217], [0.7847937847190122])\n",
      "0.7891922454115212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 5.568796936848393e-08\n",
      "Bias                              : 1.484\n",
      "Words inequality                  : -0.5876\n",
      "Words stem inequality             : 0.1073\n",
      "Word length difference            : -0.1195\n",
      "Levenshtein distance between words: 0.769\n",
      "Levenshtein distance word stems   : 0.617\n",
      "Word similarity                   : -1.8477\n",
      "Insertion                         : 0.4503\n",
      "Deletion                          : 0.1034\n",
      "Stop-word                         : 0.1412\n",
      "IsArticle                         : 0.0\n",
      "IsNegation                        : -0.0015\n",
      "None\n",
      "AUC test ([0.7648230255822491], [0.7829065542370818])\n",
      "AUC train ([0.7741282974308911], [0.7829968844402373])\n",
      "1.399889358297225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 5.973823242459237e-06\n",
      "Bias                              : 1.3402\n",
      "Words inequality                  : -0.5513\n",
      "Words stem inequality             : 0.0952\n",
      "Word length difference            : -0.4324\n",
      "Levenshtein distance between words: 0.6386\n",
      "Levenshtein distance word stems   : 0.1876\n",
      "Word similarity                   : -1.8735\n",
      "Insertion                         : 0.2611\n",
      "Deletion                          : 0.28\n",
      "Stop-word                         : 0.1554\n",
      "IsArticle                         : -0.0054\n",
      "IsNegation                        : 0.0086\n",
      "None\n",
      "AUC test ([0.7670895208744999], [0.7851200655071134])\n",
      "AUC train ([0.7762770695123234], [0.785266665890535])\n",
      "0.8040407217317165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: -1.911442219646375e-11\n",
      "Bias                              : 1.4627\n",
      "Words inequality                  : -0.5887\n",
      "Words stem inequality             : 0.1047\n",
      "Word length difference            : -0.1348\n",
      "Levenshtein distance between words: 0.7705\n",
      "Levenshtein distance word stems   : 0.6361\n",
      "Word similarity                   : -1.8263\n",
      "Insertion                         : 0.4544\n",
      "Deletion                          : 0.1043\n",
      "Stop-word                         : 0.1416\n",
      "IsArticle                         : 0.0\n",
      "IsNegation                        : -0.0014\n",
      "None\n",
      "AUC test ([0.7646850639704701], [0.7831880267590513])\n",
      "AUC train ([0.7741199226627751], [0.7834590488570656])\n",
      "1.4380442515700593\n"
     ]
    }
   ],
   "source": [
    "m.fit(X, y, probs, index_train, index_test, save_path=\"coeff_ml.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999923639321406,\n",
       " {'diff_ref': \"i***** don't* known* that* ****** gave** me**** much** ****** strength\",\n",
       "  'diff_hyp': \"****** don't* known* what* he**** gave** me**** ****** my**** friend**\",\n",
       "  'diff_cost': '-2.702 -0.205 -4.117 9.066 13.538 -4.117 -0.205 12.861 13.538 61.534  '})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(\"i don't known that gave me much strength\", \"don't known what he gave me my friend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sbs(path):\n",
    "    desc_to_votes = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    for markup in data:\n",
    "        desc = (\n",
    "            markup[\"reference\"],\n",
    "            markup[\"hypothesis_left\"],\n",
    "            markup[\"hypothesis_right\"],\n",
    "            markup[\"recognition_id_left\"],\n",
    "            markup[\"recognition_id_right\"],\n",
    "            markup[\"dataset\"],\n",
    "            markup[\"record\"],\n",
    "            markup[\"model_left\"],\n",
    "            markup[\"model_right\"],\n",
    "            str(markup[\"noise_left\"][\"level_idx\"]) if markup[\"noise_left\"] is not None else None,\n",
    "            str(markup[\"noise_right\"][\"level_idx\"]) if markup[\"noise_right\"] is not None else None,\n",
    "        )\n",
    "\n",
    "        if desc not in desc_to_votes:\n",
    "            desc_to_votes[desc] = (0, 0)\n",
    "\n",
    "        votes_total, votes_left = desc_to_votes[desc]\n",
    "\n",
    "        votes_total += 1\n",
    "        if markup[\"choice\"] == \"LEFT\":\n",
    "            votes_left += 1\n",
    "\n",
    "        desc_to_votes[desc] = (votes_total, votes_left)\n",
    "    return [{\n",
    "        \"reference\": desc[0],\n",
    "        \"hypothesis_left\": desc[1],\n",
    "        \"hypothesis_right\": desc[2],\n",
    "        \"votes_total\": desc_to_votes[desc][0],\n",
    "        \"votes_left\": desc_to_votes[desc][1],\n",
    "    } for desc in desc_to_votes]\n",
    "\n",
    "def prepare_data_sbs():\n",
    "    data = aggregate_sbs(\"votes_sbs_raw.json\")\n",
    "    alpha = 0\n",
    "    X = []\n",
    "    y = []\n",
    "    probs = []\n",
    "    for item in data:\n",
    "        if item[\"votes_total\"] != 5:\n",
    "            continue\n",
    "        persent = (item[\"votes_left\"] + alpha) / (item[\"votes_total\"] + 2 * alpha)\n",
    "        y.append(1 if persent < 0.5 else -1)\n",
    "        probs.append([persent, 1 - persent])\n",
    "    \n",
    "        sample = (item[\"reference\"], item[\"hypothesis_left\"], item[\"hypothesis_right\"])\n",
    "    \n",
    "        X.append(sample)\n",
    "    return X, y, probs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, probs = prepare_data_sbs()\n",
    "# size = 2000\n",
    "# X, y, probs = X[:size], y[:size], probs[:size]\n",
    "index_train, index_test = train_test_split(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 26622\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset size: {len(X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: -1.752764600126966e-14\n",
      "Bias            : -0.2297\n",
      "Words inequality: 1.6268\n",
      "None\n",
      "AUC test ([0.8200394303292436], [0.8355162589393816])\n",
      "AUC train ([0.8188009004285949], [0.8267146462458018])\n",
      "1.603657893237269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: -5.6295246242399344e-14\n",
      "Bias            : -0.2354\n",
      "Words inequality: 1.6337\n",
      "None\n",
      "AUC test ([0.819220245913219], [0.83489163168746])\n",
      "AUC train ([0.8187603701134456], [0.82672117154891])\n",
      "0.0072697307194708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: -5.6295246242399344e-14\n",
      "Bias            : -0.2354\n",
      "Words inequality: 1.6337\n",
      "None\n",
      "AUC test ([0.8192387732574637], [0.8349740445812472])\n",
      "AUC train ([0.8188029303003121], [0.8266894025891252])\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"WER\")\n",
    "m = MERa(\"en\", \"white_list\", features={\n",
    "    \"Bias\",\n",
    "    \"Words inequality\"\n",
    "})\n",
    "m.fit(X, y, probs, index_train, index_test, mode=\"sbs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MERa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MERa(\"en\", \"white_list\", features={\n",
    "    \"Bias\",\n",
    "    \"Words inequality\",\n",
    "    \"Insertion\",\n",
    "    \"Deletion\",\n",
    "    \"Words stem inequality\",\n",
    "    \"Word length difference\",\n",
    "    \"Levenshtein distance between words\",\n",
    "    \"Levenshtein distance word stems\",\n",
    "    \"Stop-word\",\n",
    "    \"Word similarity\",\n",
    "    \"IsArticle\",\n",
    "    \"IsNegation\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: -1.2592921497245335e-13\n",
      "Bias                              : 0.1652\n",
      "Words inequality                  : 0.2248\n",
      "Words stem inequality             : -0.2373\n",
      "Word length difference            : -0.548\n",
      "Levenshtein distance between words: 1.1422\n",
      "Levenshtein distance word stems   : -0.3121\n",
      "Word similarity                   : -0.8831\n",
      "Insertion                         : 0.1543\n",
      "Deletion                          : 0.2781\n",
      "Stop-word                         : -0.0165\n",
      "IsArticle                         : -0.0326\n",
      "IsNegation                        : 0.0159\n",
      "None\n",
      "AUC test ([0.8320726982676984], [0.8470487808472353])\n",
      "AUC train ([0.8305882148607704], [0.837754277415063])\n",
      "1.6440092443514494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 5.447861564468844e-06\n",
      "Bias                              : 0.487\n",
      "Words inequality                  : 0.0496\n",
      "Words stem inequality             : -0.3178\n",
      "Word length difference            : -1.1603\n",
      "Levenshtein distance between words: 2.4735\n",
      "Levenshtein distance word stems   : -0.3374\n",
      "Word similarity                   : -0.8431\n",
      "Insertion                         : 0.2936\n",
      "Deletion                          : 0.2269\n",
      "Stop-word                         : 0.0066\n",
      "IsArticle                         : -0.0033\n",
      "IsNegation                        : 0.0174\n",
      "None\n",
      "AUC test ([0.8356884483785381], [0.8502209575242529])\n",
      "AUC train ([0.8345566896672743], [0.8419358267917665])\n",
      "1.2127116316103532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 3.9561664792245446e-08\n",
      "Bias                              : 0.1817\n",
      "Words inequality                  : -0.012\n",
      "Words stem inequality             : -0.2732\n",
      "Word length difference            : -0.6493\n",
      "Levenshtein distance between words: 1.6949\n",
      "Levenshtein distance word stems   : -0.1138\n",
      "Word similarity                   : -0.7257\n",
      "Insertion                         : 0.2723\n",
      "Deletion                          : 0.1276\n",
      "Stop-word                         : 0.0542\n",
      "IsArticle                         : 0.005\n",
      "IsNegation                        : 0.0182\n",
      "None\n",
      "AUC test ([0.8367366701247538], [0.8507658463763725])\n",
      "AUC train ([0.8353473788874382], [0.8425641008814695])\n",
      "0.2834313133048748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 2.0425889848605088e-08\n",
      "Bias                              : 0.1922\n",
      "Words inequality                  : -0.0799\n",
      "Words stem inequality             : -0.2487\n",
      "Word length difference            : -0.4595\n",
      "Levenshtein distance between words: 1.4701\n",
      "Levenshtein distance word stems   : -0.1229\n",
      "Word similarity                   : -0.8033\n",
      "Insertion                         : 0.2682\n",
      "Deletion                          : 0.1725\n",
      "Stop-word                         : 0.0481\n",
      "IsArticle                         : -0.0017\n",
      "IsNegation                        : 0.0179\n",
      "None\n",
      "AUC test ([0.8366916689090818], [0.8502189978153976])\n",
      "AUC train ([0.835569288317364], [0.8429530275099808])\n",
      "0.1928036150826875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 7.736645436045804e-09\n",
      "Bias                              : 0.1829\n",
      "Words inequality                  : -0.0968\n",
      "Words stem inequality             : -0.2503\n",
      "Word length difference            : -0.4466\n",
      "Levenshtein distance between words: 1.4455\n",
      "Levenshtein distance word stems   : -0.1331\n",
      "Word similarity                   : -0.8245\n",
      "Insertion                         : 0.2633\n",
      "Deletion                          : 0.1857\n",
      "Stop-word                         : 0.0513\n",
      "IsArticle                         : -0.007\n",
      "IsNegation                        : 0.0162\n",
      "None\n",
      "AUC test ([0.8368855056743391], [0.8510161695530738])\n",
      "AUC train ([0.8355916030817512], [0.8428331257304023])\n",
      "0.0346092980813137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: -6.822870041297766e-14\n",
      "Bias                              : 0.1872\n",
      "Words inequality                  : -0.0993\n",
      "Words stem inequality             : -0.2577\n",
      "Word length difference            : -0.4503\n",
      "Levenshtein distance between words: 1.4614\n",
      "Levenshtein distance word stems   : -0.1361\n",
      "Word similarity                   : -0.8159\n",
      "Insertion                         : 0.2679\n",
      "Deletion                          : 0.1792\n",
      "Stop-word                         : 0.0504\n",
      "IsArticle                         : -0.0069\n",
      "IsNegation                        : 0.0175\n",
      "None\n",
      "AUC test ([0.8364194219985028], [0.8511093621679311])\n",
      "AUC train ([0.8355614130794454], [0.8428611222804724])\n",
      "0.016226142509755157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard weights\n",
      "\n",
      "sent const: 1.3499422170111085e-09\n",
      "Bias                              : 0.1865\n",
      "Words inequality                  : -0.101\n",
      "Words stem inequality             : -0.2552\n",
      "Word length difference            : -0.4414\n",
      "Levenshtein distance between words: 1.4504\n",
      "Levenshtein distance word stems   : -0.1354\n",
      "Word similarity                   : -0.82\n",
      "Insertion                         : 0.2657\n",
      "Deletion                          : 0.181\n",
      "Stop-word                         : 0.0499\n",
      "IsArticle                         : -0.0068\n",
      "IsNegation                        : 0.0174\n",
      "None\n",
      "AUC test ([0.8364313089952694], [0.8503927500808526])\n",
      "AUC train ([0.8353149649811369], [0.8427072659564985])\n",
      "0.011827923699301248\n"
     ]
    }
   ],
   "source": [
    "m.fit(X, y, probs, index_train, index_test, mode=\"sbs\", save_path=\"coeff_sbs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9998455834989539,\n",
       " {'diff_ref': \"i***** don't* known** that** ****** gave*** me**** much** ****** strength\",\n",
       "  'diff_hyp': \"****** don't* known** what** he**** gave*** me**** ****** my**** friend**\",\n",
       "  'diff_cost': '14.394 -5.796 -26.136 20.926 12.398 -26.136 -5.796 10.569 12.398 59.663  '})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(\"i don't known that gave me much strength\", \"don't known what he gave me my friend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mera(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        features = []\n",
    "        weights = []\n",
    "        for line in f:\n",
    "            feature_name, weight = line.split(\":\")\n",
    "            features.append(feature_name.strip())\n",
    "            weights.append(float(weight.strip()))\n",
    "    mera = MERa(\"en\", \"white_list\", features=features[1:])        \n",
    "    mera.weights = np.array(weights)\n",
    "    return mera\n",
    "\n",
    "def read_check_data(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    result = {\n",
    "        \"dataset\": [],\n",
    "        \"model\": [],\n",
    "        \"record\": [],\n",
    "        \"noise_level\": [],\n",
    "        \"variant\": [],\n",
    "        \"reference\": [],\n",
    "        \"hypothesis\": []\n",
    "    }\n",
    "    \n",
    "    for item in data:\n",
    "        result[\"dataset\"].append(item[\"dataset\"])\n",
    "        result[\"model\"].append(item[\"model\"])\n",
    "        result[\"record\"].append(item[\"record\"])\n",
    "        result[\"noise_level\"].append(item[\"noise\"][\"level\"] if item[\"noise\"] else None)\n",
    "        result[\"variant\"].append(item[\"noise\"][\"variant\"] if item[\"noise\"] else None)\n",
    "        result[\"reference\"].append(item[\"reference\"])\n",
    "        result[\"hypothesis\"].append(item[\"hypothesis\"])\n",
    "        \n",
    "    return result    \n",
    "\n",
    "def read_sbs_data(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    result = {\n",
    "        \"dataset\": [],\n",
    "        \"model\": [],\n",
    "        \"record\": [],\n",
    "        \"noise_level\": [],\n",
    "        \"variant\": [],\n",
    "        \"reference\": [],\n",
    "        \"hypothesis\": []\n",
    "    }\n",
    "    \n",
    "    for item in data:\n",
    "        for side in {\"left\", \"right\"}:\n",
    "            result[\"dataset\"].append(item[\"dataset\"])\n",
    "            result[\"model\"].append(item[f\"model_{side}\"])\n",
    "            result[\"record\"].append(item[\"record\"])\n",
    "            result[\"noise_level\"].append(item[f\"noise_{side}\"][\"level\"] if item[f\"noise_{side}\"] else None)\n",
    "            result[\"variant\"].append(item[f\"noise_{side}\"][\"variant\"] if item[f\"noise_{side}\"] else None)\n",
    "            result[\"reference\"].append(item[\"reference\"])\n",
    "            result[\"hypothesis\"].append(item[f\"hypothesis_{side}\"])\n",
    "        \n",
    "    return result  \n",
    "\n",
    "def for_plots(raw_data_path, saved_coeff_path, result_path, mode):\n",
    "    if mode == \"check\":\n",
    "        data = read_check_data(raw_data_path)\n",
    "    elif mode == \"sbs\":\n",
    "        data = read_sbs_data(raw_data_path)\n",
    "    else:\n",
    "        print(\"Incorrect mode\")\n",
    "    m = read_mera(\"coeff_ml.json\")\n",
    "    mers = []\n",
    "    for i in range(len(data[\"reference\"])):\n",
    "        mers.append(m(data[\"reference\"][i], data[\"hypothesis\"][i]))\n",
    "        \n",
    "    data[\"mer\"] = mers\n",
    "    with open(result_path, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_plots(\"votes_check_raw.json\", \"coeff_ml.json\", \"mera.json\", mode=\"check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_plots(\"votes_sbs_raw.json\", \"coeff_sbs.json\", \"mera_sbs.json\", mode=\"sbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
