{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookId":"e6ca2bee-ab1a-41c1-98d4-c9a5cd75ad59","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"name":"python","nbconvert_exporter":"python"}},"cells":[{"cell_type":"markdown","source":"## Setup environment","metadata":{"cellId":"v5wezd3f2wbsoo1djpz8oj"}},{"cell_type":"code","source":"%pip install mmdet==2.13.0\n%pip install Cython==0.29.5\n%pip install torch==1.9.0 torchvision==0.10.0\n%pip install nemo_toolkit['all']\n%pip install ffmpeg","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"cellId":"fb9bwdp5p8k7r9ctwhu32n","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport glob\nimport json\nimport wget\nimport copy\nimport scipy\nimport ffmpeg\nimport tarfile\nimport zipfile\nimport numpy as np \nimport pandas as pd\n\nimport librosa\nfrom librosa.display import waveplot\nimport IPython.display as ipd\n\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nimport nemo\nimport nemo.collections.asr as nemo_asr\nfrom nemo.collections.asr.metrics.wer import WER, word_error_rate\nfrom nemo.collections.asr.models import EncDecCTCModel\nimport torch \n\nfrom tqdm import tqdm\nfrom shutil import copy\nfrom scipy.io import wavfile\nfrom omegaconf import DictConfig\nfrom contextlib import contextmanager","metadata":{"cellId":"47g3zuthmf872ff3od1scf","trusted":true},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## Functions for dataset extraction and augmentation","metadata":{"cellId":"qv872jqfxymums6w1hofi9"}},{"cell_type":"code","source":"def download_extract_libri(data_dir, dataset_name):\n    '''\n    downloading the tar file with dataset and extract it into the directory\n    '''\n    if not os.path.exists(os.path.join(data_dir, f'{dataset_name}.tar.gz')):\n        libri_url = f'http://www.openslr.org/resources/12/{dataset_name}.tar.gz'\n        libri_path = wget.download(libri_url, data_dir)\n        print(f\"Dataset in .tar format downloaded at: {libri_path}\")\n    else:\n        print(\"Tarfile already exists\")\n        libri_path = os.path.join(data_dir, f'{dataset_name}.tar.gz')\n    extracted_dir = os.path.join(data_dir, 'Librispeech')\n    if not os.path.exists(extracted_dir):\n        tar = tarfile.open(libri_path)\n        tar.extractall(path=data_dir)\n        print(f\"Tarfile extracted in {extracted_dir}\")\n    else:\n        print(f\"Tarfile already extracted in {extracted_dir}\")\n        \ndef flac2wav(data_dir, dataset_name):\n    '''\n    converting flac to wav\n    '''\n    print('Converting flac to wav....')\n    flac_list = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.flac'))  \n    for flac_path in tqdm(flac_list, position=0, leave=False):\n        wav_path = flac_path[:-5] + '.wav'\n        os.system(f'ffmpeg -i {flac_path} {wav_path}')\n        \n    wav_files = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.wav'))\n    broken_files = [file for file in wav_files if os.path.getsize(file) < 100]\n    assert len(broken_files) == 0\n    for flac_path in flac_list:\n        os.remove(flac_path)\n    print('Finished')\n    \ndef build_manifest(data_dir, dataset_name, manifest_path=os.getcwd() + '/LibriSpeech/manifest.json'):\n    '''\n    build_manifest(for training)\n    '''\n    print('Building manifest...')\n    \n    transcripts = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.txt'))\n    with open(manifest_path, 'w') as out_file:\n        for trans_path in tqdm(transcripts, position=0, leave=False):\n            with open(trans_path, 'r') as file:\n                for line in file.readlines():   \n                    transcript = line.lower()[:-1]\n                    audio_name = transcript.split(' ')[0]\n                    text = transcript[len(transcript.split(' ')[0]) + 1:]\n                    path_to_folder, _ = trans_path.rsplit('/', 1)\n                    audio_path = os.path.join(path_to_folder, audio_name+'.wav')\n                    duration = librosa.core.get_duration(filename=audio_path)\n                    \n                    metadata = {\n                        \"audio_filepath\": audio_path,\n                        \"duration\": duration,\n                        \"text\": text\n                    }\n                    json.dump(metadata, out_file)\n                    out_file.write('\\n')\n    print('Finished')\n\n@contextmanager\ndef autocast(enabled=None):\n    yield\n    \ndef transcribe_audios(asr_model):\n    '''\n    Transcribe audios from created manifest file\n    '''\n    asr_model = asr_model.cuda()\n    asr_model.eval()\n    labels_map = dict([(i, asr_model.decoder.vocabulary[i]) for i in range(len(asr_model.decoder.vocabulary))])\n    wer = WER(vocabulary=asr_model.decoder.vocabulary)\n    hypotheses = []\n    references = []\n    for test_batch in asr_model.test_dataloader():\n        if torch.cuda.is_available():\n            test_batch = [x.cuda() for x in test_batch]\n        with autocast():\n            log_probs, encoded_len, greedy_predictions = asr_model(\n                input_signal=test_batch[0], input_signal_length=test_batch[1]\n            )\n        hypotheses += wer.ctc_decoder_predictions_tensor(greedy_predictions)\n        for batch_ind in range(greedy_predictions.shape[0]):\n            reference = ''.join([labels_map[c] for c in test_batch[2][batch_ind].cpu().detach().numpy()])\n            references.append(reference)\n        del test_batch\n    wer_value = word_error_rate(hypotheses=hypotheses, references=references)\n    return hypotheses, references, wer_value","metadata":{"cellId":"njcdjkebjjrmte3bayfp6m","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class NoiseAgumentator:\n    def __init__(self, dir_noises):\n        self.noises = [(file_path.rsplit('/',1)[1].split('.')[0], librosa.load(file_path, sr=16000)[0])  # np.array(scipy.io.wavfile.read(file_path)[1], dtype=np.float32))\n                       for file_path in glob.glob(dir_noises +'/*.wav')]\n\n    def normalize_audio(self, audio):\n        max_amp = max(abs(audio))\n        audio = audio * 0.73 / max_amp\n        audio = audio * 32768\n        return audio.astype(np.int16)\n                 \n    def apply_noise(self, source_audio, noise_audio, weight):\n        source_audio = self.normalize_audio(source_audio)\n        noise_audio = self.normalize_audio(noise_audio)\n        if len(source_audio) < len(noise_audio):\n            noise_audio = noise_audio[:len(source_audio)]\n        else:\n            noise_audio = np.pad(noise_audio, (0, len(source_audio) - len(noise_audio)))\n        return self.normalize_audio((1.0 - weight) * source_audio + weight * noise_audio)\n                        \n    def make_noisy(self, audio, weights=[0.1], poison_coef=1):\n        added_noises_idx = []\n        # audio = np.array(audio, dtype=np.float32)\n        noised_audios = len(weights) * [audio]\n        total = 1 + np.random.poisson(poison_coef)\n        i = 0\n        while i < total :\n            noise_idx, noise = self.noises[np.random.randint(0, len(self.noises) - 1)]\n            added_noises_idx.append(noise_idx)\n            # noise = np.resize(noise, len(audio))\n            for idx, noise_weight in enumerate(weights):\n                # noised_audios[idx] = (1.0 - noise_weight) * noised_audios[idx] + noise_weight * noise\n                noised_audios[idx] = self.apply_noise(noised_audios[idx], noise, noise_weight)\n            i += 1\n        return noised_audios, added_noises_idx\n    \ndef copy_directory_folders(inputpath, outputpath):\n    \"\"\"\n    Copy folders from one directory to another\n    \"\"\"\n    if not os.path.isdir(outputpath):\n        os.mkdir(outputpath)\n    for dirpath, dirnames, filenames in os.walk(inputpath):\n        structure = os.path.join(outputpath, os.path.relpath(dirpath, inputpath))\n        if not os.path.isdir(structure):\n            os.mkdir(structure)\n            \ndef copy_dir_txts_to_aug_folders(data_dir, dataset_name, noise_levels_idx, variants_per_noise_level):\n    \"\"\"\n    Copy folders from one directory to another and txt files containing references\n    \"\"\"\n    transcripts = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.txt'))\n    inputpath = os.path.join(data_dir, 'LibriSpeech', dataset_name)\n    for noise_level_idx in noise_levels_idx:\n        for variant in range(variants_per_noise_level):\n            noise_dataset_name = f'{dataset_name}_noised_{noise_level_idx}_variant_{variant}'\n            outputpath = os.path.join(data_dir, 'LibriSpeech', noise_dataset_name)\n            copy_directory_folders(inputpath, outputpath)\n\n            for transcript_path in transcripts:\n                l, r = transcript_path.split(f\"/{dataset_name}/\")\n                path_to_copy = os.path.join(l, noise_dataset_name ,r.rsplit('/', 1)[0])\n                copy(transcript_path, path_to_copy)   \n            \ndef aug_noise_datasets(dataset_name, noise_levels_idx, noise_levels, variants_per_noise_level):\n    \"\"\"\n    Augument audio data from one folder - dataset_name to others with the specific coefs of noises\n    \"\"\"\n    audios = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.wav'))\n    noises_info = []\n    for audio_path in tqdm(audios, position=0, leave=False):\n        sr, audio = wavfile.read(audio_path)\n        assert sr == 16000\n        part1, part2 = audio_path.split(dataset_name)\n        for variant in range(variants_per_noise_level):\n            noised, noises_idx = aug.make_noisy(audio, noise_levels)      \n            for i, noise_level_idx in enumerate(noise_levels_idx):\n                audio_noised = noised[i]\n                audio_noised_path = f'{part1}{dataset_name}_noised_{noise_level_idx}_variant_{variant}{part2.split(\".\")[0]}.wav'\n                wavfile.write(audio_noised_path, sr, audio_noised)\n            # dataset, audio_name, variant, noises\n            noises_info.append((dataset_name, part2.rsplit('/', 1)[1], variant, noises_idx))\n    \n    return noises_info","metadata":{"cellId":"trclxkwvhxvh4i67sqvl","trusted":true},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Downloading of LibriSpeech datasets: dev-other, test-other\n\nmore dataset by link: http://www.openslr.org/12/","metadata":{"cellId":"hxenps129by5jblz974z"}},{"cell_type":"code","source":"data_dir = os.getcwd()\ndataset_names = ['dev-other', 'test-other']","metadata":{"cellId":"tx1rxhwg1advphiaia05o","trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def download_dataset(dataset_name):\n    download_extract_libri(data_dir, dataset_name)\n    flac2wav(data_dir, dataset_name)","metadata":{"cellId":"9xfajqwpptl9xb4svosnl","trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"for dataset_name in dataset_names:\n    download_dataset(dataset_name)","metadata":{"cellId":"mvo6dj1sjqelj26797d6jh","trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Downloading of noises (test dataset ~304 MB)\n\nmore noises by link: https://zenodo.org/record/2529934/files/FSDnoisy18k.audio_train.zip?download=1 ~9.2 GB","metadata":{"cellId":"zcfqspse5khhq1y54rsfka"}},{"cell_type":"code","source":"wget.download('https://zenodo.org/record/2529934/files/FSDnoisy18k.audio_test.zip?download=1')\nwith zipfile.ZipFile('FSDnoisy18k.audio_test.zip',\"r\") as zip_ref:\n    zip_ref.extractall(os.path.join(data_dir, 'noises'))","metadata":{"cellId":"a47mae2b6qhw7yctdy8jdo","trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"dir_noises = os.path.join(data_dir, 'noises', 'FSDnoisy18k.audio_test')\naug = NoiseAgumentator(dir_noises)","metadata":{"cellId":"e16n5tkoz2b5oc8yjp2j74","trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Augument data with noises","metadata":{"cellId":"vfenrp8ld6e1nfwkpussb1"}},{"cell_type":"markdown","source":"$ noised\\ audio = (1 - coef)\\cdot audio + coef \\cdot noise $","metadata":{"cellId":"5zrq20vxq0tbiot5acmzlr"}},{"cell_type":"code","source":"# Considering some noise levels among grid with step 0.25% from 0.25% to 40.0%\n\nnoise_level_step = 0.0025\nnoise_levels_count = 160\nnoise_levels_idx = list(range(noise_levels_count))\nnoise_levels = [(i + 1) * noise_level_step for i in noise_levels_idx]\n\nvariations_per_noise_level = 1  # noise at fixed level is applied to each audio in multiple ways\n\nprint(f'noise_levels first 10 {noise_levels[:10]}')\nprint(f'noise_levels last 10 {noise_levels[-10:]}')\n\nnoise_levels_idx_chosen = range(9, 79 + 1, 10)  # choose any levels of your interest\nnoise_levels_chosen = [noise_levels[i] for i in noise_levels_idx_chosen]\nprint(f'noise_levels chosen {noise_levels_chosen}')","metadata":{"cellId":"jxvz86lfh5rfys8ny6hz8m","trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"noises_info = []\nfor dataset_name in dataset_names:\n    copy_dir_txts_to_aug_folders(data_dir, dataset_name, noise_levels_idx_chosen, variations_per_noise_level)\n    noises_info += aug_noise_datasets(dataset_name, noise_levels_idx_chosen, noise_levels_chosen, variations_per_noise_level)","metadata":{"cellId":"ip4ngny3kaxep63h8jhes","trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"pd.DataFrame(\n    noises_info, \n    columns=[\"dataset\", \"record\", \"variant\", \"noises\"]\n).to_json(f'noises.json', orient=\"records\")","metadata":{"cellId":"o7ljabab52kmaz2vr87uj","trusted":true},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## Check noise effect","metadata":{"cellId":"coill7blvejt1nziwvxiv"}},{"cell_type":"code","source":"def plot(audio):\n    plt.figure(figsize=(10, 2))\n    plt.title('Waveform')\n    plt.ylabel('Amplitude')\n    waveplot(audio.astype(float))\n    \ndef get_sample_audio(dir_suffix=''):\n    audio_path = os.path.join(data_dir, f'LibriSpeech/{dataset_names[0]}{dir_suffix}/4323/13259/4323-13259-0014.wav')\n    return wavfile.read(audio_path)","metadata":{"cellId":"41slfq7w8yp0qw2p24nka6n","trusted":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":"sr, audio = get_sample_audio()\nplot(audio)\nipd.Audio(audio, rate=sr)","metadata":{"cellId":"aan6m7d9c24q9yk6sq41i","trusted":true},"outputs":[],"execution_count":58},{"cell_type":"code","source":"sr, audio = get_sample_audio('_noised_19_variant_0')\nplot(audio)\nipd.Audio(audio, rate=sr)","metadata":{"cellId":"264snuh6r7yipdzk4a8rmzs","trusted":true},"outputs":[],"execution_count":60},{"cell_type":"code","source":"sr, audio = get_sample_audio('_noised_79_variant_0')\nplot(audio)\nipd.Audio(audio, rate=sr)","metadata":{"cellId":"modz9uqlfqjp437bx9hq9b","trusted":true},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"## Building manifests for transcribing audios","metadata":{"cellId":"e37zybyu5weyvi3er1qlzk"}},{"cell_type":"code","source":"datasets = list(dataset_names)\nfor dataset_name in dataset_names:\n    for variant in range(variations_per_noise_level):\n        noised_datasets = [f'{dataset_name}_noised_{noise_level_idx}_variant_{variant}' for noise_level_idx in noise_levels_idx_chosen]\n        datasets += noised_datasets\n\nfor dataset in datasets:\n    build_manifest(data_dir, dataset,  manifest_path=os.path.join(data_dir, 'LibriSpeech', f'{dataset}_manifest.json'))","metadata":{"cellId":"fngxq95agbiuuzo3n8ddft","trusted":true},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"## Loading models: Jasper10x5Dr-En, QuartzNet15x5NR-En","metadata":{"cellId":"0kq3md5cfrfpb0vkae8bolj"}},{"cell_type":"code","source":"#!L\n# other available models\nEncDecCTCModel.list_available_models()","metadata":{"cellId":"kzocdsttkch6faekts14e","trusted":true},"outputs":[],"execution_count":63},{"cell_type":"code","source":"#!L\nmodel_names = [\"stt_en_jasper10x5dr\", \"QuartzNet15x5Base-En\"]\n\nasr_models = [EncDecCTCModel.from_pretrained(model_name=model_name) for model_name in model_names]","metadata":{"cellId":"hvib8uz85w4273s0l8bla6","trusted":true},"outputs":[],"execution_count":66},{"cell_type":"code","source":"#!L\nrecognitions_data = []\nwers_data = []\nfor model_idx, asr_model in enumerate(asr_models):\n    for dataset in datasets:\n        model_name = model_names[model_idx]\n        print(f'model {model_name} dataset {dataset}', flush=True)\n        \n        manifest_filepath = os.path.join(data_dir,  'LibriSpeech', f'{dataset}_manifest.json')\n        asr_model.setup_test_data(\n            test_data_config={\n                'sample_rate': 16000,\n                'manifest_filepath': manifest_filepath,\n                'labels': asr_model.decoder.vocabulary,\n                'batch_size': 4,\n                'normalize_transcripts': True,\n            }\n        )\n        \n        with open(manifest_filepath) as f:\n            audios_names = [\n                json.loads(line)['audio_filepath'].split(dataset)[1].rsplit('/', 1)[1] \n                for line in f.read().split('\\n') if len(line) > 0\n            ]\n    \n        if dataset in dataset_names:\n            dataset_name = dataset\n            noise_level_idx = None\n            noise_level = None\n            variant = None\n        else:\n            # f'{dataset_name}_noised_{i}_variant_{j}'\n            dataset_name, _, noise_level_idx, _, variant = dataset.split('_')\n            variant = int(variant)\n            noise_level_idx = int(noise_level_idx)\n            noise_level = noise_levels[noise_levels_idx.index(noise_level_idx)]\n\n        hyps, refs, wer = transcribe_audios(asr_model)\n        for audio_index, hyp_ref in enumerate(zip(hyps, refs)):\n            hyp, ref = hyp_ref\n            audio_name = audios_names[audio_index]\n            recognitions_data.append((\n                dataset_name, model_name, audio_name, noise_level_idx, variant, ref, hyp\n            ))\n        \n        \n        wers_data.append((\n            dataset_name, model_name, noise_level, variant, wer\n        ))","metadata":{"cellId":"7xjfd4qntzhoyjo9v091t","trusted":true},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":"## Save dataset","metadata":{"cellId":"su6d9b4h5fiuuabkzrvelq"}},{"cell_type":"code","source":"pd.DataFrame(\n    recognitions_data, \n    columns=['dataset', 'model', 'record', 'noise_level', 'variant', 'reference', 'hypothesis']\n).to_json(f'recognitions.json', orient=\"records\")\n        \n    \nwer_df = pd.DataFrame(\n    wers_data,\n    columns=['dataset', 'model', 'noise_level', 'variant', 'wer']\n)\nwer_df.to_json(f'wers.json', orient=\"records\")","metadata":{"cellId":"asr4t6xwoho13pk86rs3ur","trusted":true},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"## (Noise level -> WER) plots","metadata":{"cellId":"a311rn26ax0tiv5eyroolk"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_wer_sample():\n    for dataset in dataset_names:\n        ax = plt.gca(title=dataset)\n        dataset_df = wer_df[wer_df['dataset'] == dataset][wer_df['variant'] == 0]\n        for model in model_names:\n            model_df = dataset_df[dataset_df['model'] == model]\n            model_df.plot(kind='line', x='noise_level', y='wer', label=model, ax=ax)\n        plt.show()  \n    \nplot_wer_sample()","metadata":{"cellId":"ubecsvzoi5rzbyma4244wr","trusted":true},"outputs":[],"execution_count":70},{"cell_type":"code","source":"","metadata":{"cellId":"icoedlrlh5qowlk9qg7w28"},"outputs":[],"execution_count":null}]}