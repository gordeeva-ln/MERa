We use [Toloka](https://toloka.ai/) crowdsourcing platform to perform two types of human texts evaluations:

# 1. SbS (Side-by-Side)

We give assessor original LibriSpeech audio transcript (reference) with two recognition hypotheses corresponding to two different audio noise levels and ask to choose hypothesis which best conveys the meaning of the original transcript. 

By this means we compare recognitions on different noise levels pairs to choose "winner" noise level.

- [Task instructions](../instructions/en/sbs.md)
- [Task interface](../interfaces/en/sbs.png)

# 2. Meaning loss

We give assessor original LibriSpeech audio transcript (reference) and recognition hypothesis corresponding to some noise level and ask the question: "Are the meanings similar?", on which assessor can answer:
- Yes
- No
- The second sentence (recognition hypothesis) does not make sense

Votes ratio `No / (Yes + No)` for particular noise level corresponds to **meaning loss** for recognitions on this level.

- [Task instructions](../instructions/en/meaning-loss.md)
- [Task interface](../interfaces/en/meaning-loss.png)

# Evaluations quality control

To control assessors answers quality we add control tasks to task assignments along with real tasks. For control tasks we know correct answers in advance, assignments with incorrect answers to control tasks were rejected and the corresponding evaluations were not taken into account.

Control tasks were made up as follows:
- SbS: we ask to compare hypotheses of significantly different noise levels, i.e. 0% and 30% expecting that assessor will choose hypothesis of less noise level (0%).
- Meaning loss: we give reference text with hypothesis corresponging to high noise level, i.e. 30%, expecting that meaning of hypothesis text will be lost on this level and assessor will ask "No" to task question.